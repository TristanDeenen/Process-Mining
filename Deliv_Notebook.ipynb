{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBL Process Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Class definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CPU usage\n",
    "\n",
    "import psutil\n",
    "p = psutil.Process()\n",
    "p.cpu_percent(interval=None)\n",
    "#all of the code you want to check under here\n",
    "#p.cpu_percent(interval=None) at the end\n",
    "#multithreading causes over 100% usage\n",
    "\n",
    "\n",
    "#define global variables\n",
    "util_counter = 0\n",
    "memory_stacked = 0\n",
    "cpu_stacked = 0\n",
    "\n",
    "memory_initial = round(psutil.virtual_memory()[3]/1024/1024/1024, 3)\n",
    "memory_initial_perc = psutil.virtual_memory()[2]\n",
    "\n",
    "cpu_initial_perc = psutil.cpu_percent(4)\n",
    "# Define function\n",
    "\n",
    "def reset_mem_report():\n",
    "    util_counter = 0\n",
    "    memory_stacked = 0\n",
    "    cpu_stacked = 0\n",
    "\n",
    "def mem_report():\n",
    "    #retrieve global variables (counters and initial values)\n",
    "    global util_counter\n",
    "    global memory_stacked\n",
    "    global cpu_stacked\n",
    "    \n",
    "    global memory_initial\n",
    "    global memory_initial_perc\n",
    "    \n",
    "    global cpu_initial_perc\n",
    "    \n",
    "    #retrieve current utils\n",
    "    ram = psutil.virtual_memory()[3]\n",
    "    ram_perc = psutil.virtual_memory()[2]\n",
    "    cpu_perc = psutil.cpu_percent(4)\n",
    "    \n",
    "    #print RAM usage\n",
    "    print('Initial RAM memory usage:', memory_initial, 'GB')\n",
    "    print('Current RAM memory usage:', round(ram/1024/1024/1024, 3), 'GB\\n')\n",
    "    \n",
    "    print('Initial RAM memory % used:', memory_initial_perc, '%')\n",
    "    print('Current RAM memory % used:', ram_perc, '%\\n')\n",
    "    \n",
    "    #add values to sums, and update counter\n",
    "    util_counter += 1\n",
    "    memory_stacked += ram_perc\n",
    "    cpu_stacked += cpu_perc\n",
    "    \n",
    "    print('Averaged RAM memory % used', memory_stacked/util_counter, '%\\n')\n",
    "    \n",
    "    #print CPU usage\n",
    "    print('Initial CPU % used: ', cpu_initial_perc, '%')\n",
    "    print('Current CPU % used: ', cpu_perc, '%\\n')\n",
    "    print('Averaged CPU % used', cpu_stacked/util_counter, '%')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Utils.LogFile import LogFile \n",
    "import tensorflow as tf\n",
    "import multiprocessing as mp\n",
    "import copy\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, LSTM\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two different methods: \n",
    "- One csv file, which still has to be split into training and test data\n",
    "- Two csv files, which are already split into training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define attribute columns here\n",
    "case_attr = \"Case ID\"\n",
    "act_attr = \"concept:name\"\n",
    "time_attr = \"Complete Timestamp\"\n",
    "path = \"data/BPI_Challenge_2012_end.csv\"\n",
    "time_format = '%Y-%m-%d %H:%M:%S.%f'\n",
    "cycle_attr = 'lifecycle:transition' # Optional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logfile = LogFile(path, \",\", 0, None, time_attr=time_attr, trace_attr=case_attr,\n",
    "                   activity_attr=act_attr, time_format=time_format, cycle_attr=cycle_attr, convert=False, k=20)\n",
    "#logfile = logfile.create_subset(40)\n",
    "#logfile.add_end_events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(logfile.data) / len(logfile.data[logfile.trace].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logfile.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_log = logfile.create_split_df()\n",
    "split_train, split_test = split_log.split_train_test(range(67, 73), type='normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_date = split_train.data['Complete Timestamp'].max()\n",
    "split_case = split_train.data['Case ID'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, sharex=True, figsize=(20, 22))\n",
    "\n",
    "sns.scatterplot(data=split_log.data, x=split_log.time, y=split_log.trace, hue=split_log.activity, ax=ax[0])\n",
    "sns.scatterplot(data=split_train.data, x=split_log.time, y=split_log.trace, ax=ax[1])\n",
    "sns.scatterplot(data=split_test.data, x=split_log.time, y=split_log.trace, ax=ax[1])\n",
    "\n",
    "fig.suptitle('Visualization of train-test split', size=25, weight='bold', y=1.01)\n",
    "fig.tight_layout()\n",
    "\n",
    "ax[0].set_xlabel('')\n",
    "ax[0].set_ylabel('Case ID')\n",
    "ax[1].set_xlabel('Date')\n",
    "ax[1].set_ylabel('Case ID')\n",
    "\n",
    "ax[0].axvline(x=split_date, color = '#404040', linestyle='--', linewidth=2)\n",
    "ax[0].axhline(y=split_case, color = '#404040', linestyle='--', linewidth=2)\n",
    "ax[1].axvline(x=split_date, color = '#404040', linestyle='--', linewidth=2)\n",
    "ax[1].axhline(y=split_case, color = '#404040', linestyle='--', linewidth=2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logfile.keep_attributes([logfile.trace, logfile.time, logfile.activity, logfile.cycle_attr])\n",
    "activity_map = logfile.int_convert()\n",
    "logfile.add_start_date()\n",
    "logfile.create_k_context()\n",
    "log_train, log_test = logfile.split_train_test(range(67, 73), type='normal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_time(dataset):\n",
    "    \"\"\"Adds a new column to a dataset with the converted timestamp to datetime\"\"\"\n",
    "\n",
    "    date_list = []\n",
    "\n",
    "    for time in dataset[logfile.time]:\n",
    "        date = datetime.strptime(time, logfile.time_format)\n",
    "        date_list.append(date)\n",
    "\n",
    "    dataset['time and date'] = date_list\n",
    "mem_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add actual next event and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_actual_next(df_case):\n",
    "    \"\"\"Adds the actual next activity and time to next event to the final dataframe\"\"\"\n",
    "\n",
    "    # Create a list for all the actual next events for an case\n",
    "    event_lst = [event for event in df_case[logfile.activity]] # Gets a list of all events for a specific trace\n",
    "    event_lst = event_lst[1:] # Erase the first activity from the list (thus the second activity becomes first in the list)\n",
    "    event_lst.append('-') # Append a '-' to the end of the list (the last activity does not have a next activity)\n",
    "    \n",
    "    # Create a list for time of the next event\n",
    "    nexttime_lst1 = [time for time in df_case['time and date']]\n",
    "    nexttime_lst = nexttime_lst1[1:]\n",
    "    nexttime_lst.append(nexttime_lst[-1])\n",
    "\n",
    "    # Create the time difference list\n",
    "    time_diff = []\n",
    "    for i in range(len(nexttime_lst)):\n",
    "        time_diff.append((nexttime_lst[i] - nexttime_lst1[i]).total_seconds())\n",
    "\n",
    "    # Append columns to the case dataframe\n",
    "    df_case['Next event'] = event_lst\n",
    "    df_case['Time to next event'] = time_diff\n",
    "\n",
    "    trace_len = len(df_case)\n",
    "\n",
    "    return trace_len\n",
    "mem_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicted next event and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_position_time(df_case, count_dict, time_dict):\n",
    "    for index, row in df_case.iterrows():\n",
    "        \n",
    "        # Get the amount of times an action occured in a certain position {action : {position_1 : count_1, position_2: count_2}}\n",
    "        if row[logfile.activity] in count_dict:\n",
    "            if index in count_dict[row[logfile.activity]]:\n",
    "                count_dict[row[logfile.activity]][index] += 1\n",
    "            else:\n",
    "                count_dict[row[logfile.activity]].update({index: 1})\n",
    "        else:\n",
    "            count_dict[row[logfile.activity]] = {index: 1}\n",
    "        \n",
    "        # Summation of the times to next action per position (index) {position: {\"sum\": summation_of_time, \"count\": amount_of_times_occured (to calculate mean)}}\n",
    "        if index in time_dict:\n",
    "            time_dict[index]['sum'] += row['Time to next event']\n",
    "            time_dict[index]['count'] += 1\n",
    "        else:\n",
    "            time_dict[index] = {'sum': row['Time to next event'], 'count': 1}\n",
    "\n",
    "def get_position_rank(max_trace_len, count_dict):\n",
    "    pos_rank_dict = {}\n",
    "    for i in range(max_trace_len):\n",
    "        init = 0\n",
    "        task = 0\n",
    "        for key in count_dict.keys():\n",
    "            try:\n",
    "                new = count_dict[key][i]\n",
    "            except:\n",
    "                new = 0\n",
    "            if new > init:\n",
    "                init = new\n",
    "                task = key\n",
    "\n",
    "        pos_rank_dict.update({i: task})\n",
    "    \n",
    "    return pos_rank_dict\n",
    "\n",
    "def get_mean_time(total_time_dict):\n",
    "    mean_time_dict = {}\n",
    "    for position in total_time_dict.keys():\n",
    "        mean_time = total_time_dict[position]['sum'] / total_time_dict[position]['count']\n",
    "        mean_time_dict[position] = mean_time\n",
    "    \n",
    "    return mean_time_dict\n",
    "mem_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_event_pred(df_case, pos_rank_dict, mean_time_dict):\n",
    "    \n",
    "    # Prediction for the action\n",
    "    pred_act_lst = [pos_rank_dict[i] for i in range(len(df_case))]\n",
    "    pred_act_lst = pred_act_lst[1:]\n",
    "    pred_act_lst.append('-')\n",
    "\n",
    "    # Prediction for time\n",
    "    pred_time_lst = [mean_time_dict[i] for i in range(len(df_case))]\n",
    "\n",
    "    df_case['Event prediction'] = pred_act_lst \n",
    "    df_case['Time prediction'] = pred_time_lst\n",
    "mem_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_baseline(dataframe, maximum=None):\n",
    "    \"\"\"Returns the training dataset with predictions and 2 dictionaries which predict next action and nexttime based on position\"\"\"\n",
    "    \n",
    "    dataset = dataframe\n",
    "    convert_time(dataset)\n",
    "\n",
    "    df_actual = pd.DataFrame()\n",
    "\n",
    "\n",
    "    # Creating a dataframe with the actual events\n",
    "\n",
    "    cases = list(dataset[logfile.trace].unique())  \n",
    "    max_trace_len = 0  \n",
    "    pos_count_dict = {}\n",
    "    time_dict = {}\n",
    "    for case in cases[:maximum]:\n",
    "        df_case = dataset[dataset[logfile.trace] == case].copy().reset_index(drop=True)\n",
    "        trace_len = add_actual_next(df_case)\n",
    "        get_position_time(df_case, pos_count_dict, time_dict)\n",
    "        df_actual = pd.concat([df_actual, df_case])\n",
    "\n",
    "        if trace_len > max_trace_len:\n",
    "            max_trace_len = trace_len\n",
    "    \n",
    "\n",
    "\n",
    "    # Creating the predicitions\n",
    "    df_predicted = pd.DataFrame()\n",
    "    \n",
    "    pos_rank_dict = get_position_rank(max_trace_len, pos_count_dict)\n",
    "    mean_time_dict = get_mean_time(time_dict)\n",
    "\n",
    "    for case in cases[:maximum]:\n",
    "        df_case = df_actual[df_actual[logfile.trace] == case].copy().reset_index(drop=True)\n",
    "        create_event_pred(df_case, pos_rank_dict, mean_time_dict)\n",
    "        df_predicted = pd.concat([df_predicted,df_case])\n",
    "\n",
    "\n",
    "\n",
    "    return df_predicted, pos_rank_dict, mean_time_dict\n",
    "mem_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_baseline(dataframe, train_pos, train_time):\n",
    "    \"\"\"Creates the test dataset including the predictions based on the training dataset\"\"\"\n",
    "    \n",
    "    dataset = dataframe\n",
    "    convert_time(dataset)\n",
    "\n",
    "    df_predict = pd.DataFrame()\n",
    "    cases = list(dataset[logfile.trace].unique())  \n",
    "    for case in cases:\n",
    "        df_case = dataset[dataset[logfile.trace] == case].copy().reset_index(drop=True)\n",
    "        _ = add_actual_next(df_case)\n",
    "        create_event_pred(df_case, train_pos, train_time)\n",
    "        df_predict = pd.concat([df_predict, df_case])\n",
    "    \n",
    "    return df_predict\n",
    "mem_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(dataset):\n",
    "    event_accuracy = np.mean(dataset['Next event'] ==  dataset['Event prediction'])\n",
    "    time_accuracy = np.mean(abs(dataset['Time to next event'] - dataset['Time prediction'])) / 86400  # Mean Absolute Error in days\n",
    "    \n",
    "    return event_accuracy, time_accuracy\n",
    "\n",
    "def get_sample_weight(dataset):\n",
    "    sample_dict = {}\n",
    "    for event in dataset[act_attr]:\n",
    "        if event in sample_dict:\n",
    "            sample_dict[event] += 1\n",
    "        else:\n",
    "            sample_dict[event] = 1\n",
    "\n",
    "\n",
    "def get_balanced_accuracy(actual_event, event_pred, actual_time, time_pred):\n",
    "    event_balanced_accuracy = balanced_accuracy_score(actual_event, event_pred, adjusted=True) #possibly use sample weight\n",
    "    time_accuracy = np.mean(abs(actual_time - time_pred)) / 86400  # Mean Absolute Error in days\n",
    "    return event_balanced_accuracy, time_accuracy\n",
    "mem_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = log_train.data\n",
    "test_df = log_test.data\n",
    "mem_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, train_pos, train_time = train_baseline(train_df)\n",
    "test_df = test_baseline(test_df, train_pos, train_time)\n",
    "\n",
    "mem_report()\n",
    "p.cpu_percent(interval=None)\n",
    "reset_mem_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.cpu_percent(interval=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest event prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X = log_train.contextdata.copy()\n",
    "df_X.loc[df_X['Complete Timestamp_Prev0'] == 0, \"Complete Timestamp_Prev0\"] = df_X['Start Date']\n",
    "df_X['Complete Timestamp_Prev0'] = pd.to_datetime(df_X['Complete Timestamp_Prev0'])\n",
    "df_X['Complete Timestamp'] = pd.to_datetime(df_X['Complete Timestamp'])\n",
    "df_X['Start Date'] = pd.to_datetime(df_X['Start Date'])\n",
    "df_X['time_since_start'] = (df_X['Complete Timestamp_Prev0'] - df_X['Start Date']).dt.total_seconds()\n",
    "df_X['day_previous_event'] = df_X['Complete Timestamp_Prev0'].dt.weekday\n",
    "df_X['hour_previous_event'] = df_X['Complete Timestamp_Prev0'].dt.hour\n",
    "df_X['time_to_next_event'] = (df_X['Complete Timestamp'] - df_X['Complete Timestamp_Prev0']).dt.total_seconds()\n",
    "mem_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X_test = log_test.contextdata.copy()\n",
    "df_X_test.loc[df_X_test['Complete Timestamp_Prev0'] == 0, \"Complete Timestamp_Prev0\"] = df_X_test['Start Date']\n",
    "df_X_test['Complete Timestamp_Prev0'] = pd.to_datetime(df_X_test['Complete Timestamp_Prev0'])\n",
    "df_X_test['Complete Timestamp'] = pd.to_datetime(df_X_test['Complete Timestamp'])\n",
    "df_X_test['Start Date'] = pd.to_datetime(df_X_test['Start Date'])\n",
    "df_X_test['time_since_start'] = (df_X_test['Complete Timestamp_Prev0'] - df_X_test['Start Date']).dt.total_seconds()\n",
    "df_X_test['day_previous_event'] = df_X_test['Complete Timestamp_Prev0'].dt.weekday\n",
    "df_X_test['hour_previous_event'] = df_X_test['Complete Timestamp_Prev0'].dt.hour\n",
    "df_X_test['time_to_next_event'] = (df_X_test['Complete Timestamp'] - df_X_test['Complete Timestamp_Prev0']).dt.total_seconds()\n",
    "mem_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_X[logfile.activity]\n",
    "columns = ['time_since_start', 'day_previous_event', 'hour_previous_event']\n",
    "columns.extend([\"%s_Prev%i\" % (logfile.activity, i) for i in range(logfile.k)])\n",
    "columns.extend([\"%s_Prev%i\" % (logfile.cycle_attr, i) for i in range(logfile.k)])\n",
    "X = df_X[columns]\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "rf = rf.fit(X, y)\n",
    "\n",
    "df_X['rf_prediction'] = rf.predict(df_X[columns])\n",
    "df_X_test['rf_prediction'] = rf.predict(df_X_test[columns])\n",
    "\n",
    "\n",
    "accuracy_event = np.mean(df_X_test['rf_prediction'] == df_X_test[logfile.activity])\n",
    "mem_report()\n",
    "print(accuracy_event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest time prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = df_X['time_to_next_event']\n",
    "columns = ['time_since_start', 'day_previous_event', 'hour_previous_event']\n",
    "columns.extend([\"%s_Prev%i\" % (logfile.activity, i) for i in range(logfile.k)])\n",
    "columns.extend([\"%s_Prev%i\" % (logfile.cycle_attr, i) for i in range(logfile.k)])\n",
    "X2 = df_X[columns]\n",
    "\n",
    "\n",
    "rf2 = RandomForestRegressor(n_estimators = 100, random_state = 42)\n",
    "rf2 = rf2.fit(X2, y2)\n",
    "\n",
    "df_X['rf_time_prediction'] = rf2.predict(df_X[columns])\n",
    "df_X_test['rf_time_prediction'] = rf2.predict(df_X_test[columns])\n",
    "\n",
    "time_mae = np.mean(abs(df_X_test['time_to_next_event'] - df_X_test['rf_time_prediction'])) / 86400\n",
    "\n",
    "mem_report()\n",
    "p.cpu_percent(interval=None)\n",
    "reset_mem_report()\n",
    "\n",
    "\n",
    "print(time_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.cpu_percent(interval=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_log(log):\n",
    "\n",
    "    activities = np.unique(log.data[log.activity])\n",
    "    X = np.zeros((len(log.contextdata), log.k, len(activities)+ 9), dtype=np.float32)\n",
    "    y_a = np.zeros((len(log.contextdata), len(activities) + 1), dtype=np.float32)\n",
    "    y_t = np.zeros((len(log.contextdata)), dtype=np.float32)\n",
    "    j = 0\n",
    "    time_diff = 0\n",
    "    for row in log.contextdata.iterrows():\n",
    "        \n",
    "            act = getattr(row[1], log.activity)\n",
    "            event_str = getattr(row[1], log.time)\n",
    "            prev_str = getattr(row[1], \"%s_Prev0\" % (log.time))\n",
    "            start_str = getattr(row[1], \"Start Date\")\n",
    "            event_time = time.strptime(event_str, logfile.time_format)\n",
    "            start_time = time.strptime(start_str, logfile.time_format)\n",
    "\n",
    "            if prev_str != 0:\n",
    "                prev_time = time.strptime(prev_str, logfile.time_format)\n",
    "                diff_prev_event = datetime.fromtimestamp(time.mktime(event_time)) \\\n",
    "                                          - datetime.fromtimestamp(time.mktime(prev_time))\n",
    "                diff = diff_prev_event.total_seconds()\n",
    "\n",
    "            else: \n",
    "                diff = 0\n",
    "\n",
    "                        \n",
    "    \n",
    "            y_a[j, act] = 1\n",
    "            y_t[j] = diff            \n",
    "\n",
    "            k = 0\n",
    "            act_count = 0\n",
    "            last_act = None\n",
    "            for i in range(log.k -1, -1, -1):\n",
    "                \n",
    "                if getattr(row[1], \"%s_Prev%i\" % (log.activity, i)) != 0: # 0 indicates no activity (first activity is encoded to 1)\n",
    "                    X[j, log.k - i - 1, getattr(row[1], \"%s_Prev%i\" % (log.activity, i))] = 1\n",
    "                    if getattr(row[1], \"%s_Prev%i\" % (log.activity, i)) != last_act:\n",
    "                        last_act = getattr(row[1], \"%s_Prev%i\" % (log.activity, i))\n",
    "                        act_count = 0\n",
    "                    else:\n",
    "                        act_count += 1\n",
    "                else:\n",
    "                    last_act = 0\n",
    "                    act_count = 0\n",
    "\n",
    "                X[j, log.k - i - 1, len(activities)+4] = k \n",
    "                X[j, log.k - i - 1, len(activities)+2] = act_count # How many times one activity occurs in a row \n",
    "\n",
    "                if getattr(row[1], \"%s_Prev%i\" % (log.cycle_attr, i)) != 0:\n",
    "                   X[j, log.k - i - 1, len(activities)+3] = getattr(row[1], \"%s_Prev%i\" % (log.cycle_attr, i)) # Lifecycle transition state of the event\n",
    "\n",
    " \n",
    "                str_time = getattr(row[1], \"%s_Prev%i\" % (log.time, i))\n",
    "                if str_time != 0:\n",
    "                    event_time = time.strptime(str_time, logfile.time_format)\n",
    "                    time_since_start = datetime.fromtimestamp(time.mktime(event_time)) \\\n",
    "                                        - datetime.fromtimestamp(time.mktime(start_time))\n",
    "                    X[j, log.k - i - 1, len(activities) + 6] = event_time.tm_hour # Hour of day\n",
    "                    X[j, log.k - i - 1, len(activities) + 7] = event_time.tm_wday # Day of the week\n",
    "                    X[j, log.k - i - 1, len(activities) + 8] = time_since_start.total_seconds() # Seconds since start\n",
    "                else: \n",
    "                    X[j, log.k - i - 1, len(activities) + 6] = 0 \n",
    "                    X[j, log.k - i - 1, len(activities) + 7] = 0 \n",
    "                    X[j, log.k - i - 1, len(activities) + 8] = 0\n",
    "                    \n",
    "                prev_str = getattr(row[1], \"%s_Prev%i\" % (log.time, i + 1))\n",
    "                if prev_str != 0:\n",
    "                    \n",
    "                    prev_time = time.strptime(prev_str, logfile.time_format)\n",
    "                    diff_prev_event = datetime.fromtimestamp(time.mktime(event_time)) \\\n",
    "                                        - datetime.fromtimestamp(time.mktime(prev_time))\n",
    "                    time_diff = diff_prev_event.total_seconds() \n",
    "                    X[j, log.k - i - 1, len(activities) + 5] = time_diff\n",
    "                else:\n",
    "                     X[j, log.k - i - 1, len(activities) + 5] = 0\n",
    "\n",
    "                        \n",
    "\n",
    "                k += 1\n",
    "\n",
    "            j += 1\n",
    "\n",
    "    return X, y_a, y_t\n",
    "\n",
    "mem_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_LSTM(log, epochs=4, early_stop=42):\n",
    "\n",
    "\n",
    "    print(\"Transforming log...\")\n",
    "    X, y_a, y_t = transform_log(log)\n",
    "\n",
    "    # build the model:\n",
    "    print('Build model...')\n",
    "    main_input = Input(shape=(log.k, len(np.unique(log.data[log.activity]))+9), name='main_input')\n",
    "    # train a 2-layer LSTM with one shared layer\n",
    "    l1 = LSTM(100, implementation=2, kernel_initializer='glorot_uniform', return_sequences=True, dropout=0.2)(main_input) # the shared layer\n",
    "    b1 = BatchNormalization()(l1)\n",
    "    l2_1 = LSTM(100, implementation=2, kernel_initializer='glorot_uniform', return_sequences=False, dropout=0.2)(b1) # the layer specialized in activity prediction\n",
    "    b2_1 = BatchNormalization()(l2_1)\n",
    "    l2_2 = LSTM(100, implementation=2, kernel_initializer='glorot_uniform', return_sequences=False, dropout=0.2)(b1) # the layer specialized in time prediction\n",
    "    b2_2 = BatchNormalization()(l2_2)\n",
    "\n",
    "    act_output = Dense(len(np.unique(log.data[log.activity])) + 1, activation='softmax', kernel_initializer='glorot_uniform', name='act_output')(b2_1)\n",
    "    time_output = Dense(1, kernel_initializer='glorot_uniform', name='time_output')(b2_2)\n",
    "\n",
    "\n",
    "    model = Model(inputs=[main_input], outputs=[act_output, time_output])\n",
    "\n",
    "    opt = Nadam(learning_rate=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004, clipvalue=3)\n",
    "\n",
    "    model.compile(loss={'act_output':'categorical_crossentropy', 'time_output': 'mae'}, optimizer=opt)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=early_stop)\n",
    "    model_checkpoint = ModelCheckpoint(os.path.join(\"model\", 'model_{epoch:03d}-{val_loss:.2f}.h5'), monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=False, mode='auto')\n",
    "    lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "    if len(y_a) > 10:\n",
    "        split = 0.2\n",
    "    else:\n",
    "        split = 0\n",
    "\n",
    "    model.fit(X, {'act_output': y_a, 'time_output': y_t}, validation_split=split, verbose=2, callbacks=[early_stopping, lr_reducer], batch_size=log.k, epochs=epochs)\n",
    "\n",
    "    return model\n",
    "mem_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, log):\n",
    "    X, y_a, y_t = transform_log(log)\n",
    "    pred_act, pred_time = model.predict(X)\n",
    "    predict_vals = np.argmax(pred_act, axis=1)\n",
    "    pred_time = pred_time.reshape(-1)\n",
    "    #predict_probs = predictions[np.arange(predictions.shape[0]), predict_vals]\n",
    "    expected_vals = np.argmax(y_a, axis=1)\n",
    "    #expected_probs = predictions[np.arange(predictions.shape[0]), expected_vals]\n",
    "    activity_acc = np.mean(expected_vals ==  predict_vals)\n",
    "    mae_time = np.mean(abs(y_t - pred_time)) / 86400\n",
    "    return predict_vals, pred_time, activity_acc, mae_time\n",
    "mem_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_LSTM(log_train, epochs=6, early_stop=2)\n",
    "mem_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_act, pred_time, acc_act, mae_time = test(model, log_test)\n",
    "mem_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_act\n",
    "mem_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_time\n",
    "mem_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Dataset Compiling and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()\n",
    "mem_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['Next event'].replace(activity_map, inplace=True)\n",
    "test_df['Event prediction'].replace(activity_map, inplace=True)\n",
    "test_df[log_test.activity].replace(activity_map, inplace=True)\n",
    "mem_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_act = [activity_map[act] for act in pred_act]\n",
    "LSTM_act = LSTM_act[1:]\n",
    "LSTM_act.append('-')\n",
    "test_df['LSTM event prediction'] = LSTM_act\n",
    "test_df.loc[test_df[log_test.activity] == \"End\", \"LSTM event prediction\"] = \"-\"\n",
    "mem_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_time = list(pred_time[1:])\n",
    "LSTM_time.append('-')\n",
    "test_df['LSTM time prediction'] = LSTM_time\n",
    "test_df.loc[test_df[log_test.activity] == 'End', 'LSTM time prediction'] = '-'\n",
    "mem_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_act = [activity_map[act] for act in df_X_test['rf_prediction']]\n",
    "rf_act = rf_act[1:]\n",
    "rf_act.append('-')\n",
    "test_df['RF event prediction'] = rf_act\n",
    "test_df.loc[test_df[log_test.activity] == \"End\", \"RF event prediction\"] = \"-\"\n",
    "mem_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_time = list(df_X_test['rf_time_prediction'][1:])\n",
    "rf_time.append('-')\n",
    "test_df['RF time prediction'] = rf_time\n",
    "test_df.loc[test_df[log_test.activity] == 'End', 'RF time prediction'] = '-'\n",
    "mem_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df[test_df[log_test.activity] != 'End']\n",
    "mem_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_event_acc, base_time_mae = get_balanced_accuracy(test_df['Next event'], test_df['Event prediction'], test_df['Time to next event'], test_df['Time prediction'])\n",
    "LSTM_event_acc, LSTM_time_mae = get_balanced_accuracy(test_df['Next event'], test_df['LSTM event prediction'], test_df['Time to next event'], test_df['LSTM time prediction'])\n",
    "RF_event_acc, RF_time_mae = get_balanced_accuracy(test_df['Next event'], test_df['RF event prediction'], test_df['Time to next event'], test_df['RF time prediction'])\n",
    "mem_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(base_event_acc, LSTM_event_acc, RF_event_acc)\n",
    "mem_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(base_time_mae, LSTM_time_mae, RF_time_mae)\n",
    "mem_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['Time to next event'] = pd.to_datetime(test_df['Complete Timestamp']) + pd.to_timedelta(test_df['Time to next event'], unit='s')\n",
    "mem_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['Time prediction'] = pd.to_datetime(test_df['Complete Timestamp']) + pd.to_timedelta(test_df['Time prediction'], unit='s')\n",
    "test_df['LSTM time prediction'] = pd.to_datetime(test_df['Complete Timestamp']) + pd.to_timedelta(test_df['LSTM time prediction'], unit='s')\n",
    "test_df['RF time prediction'] = pd.to_datetime(test_df['Complete Timestamp']) + pd.to_timedelta(test_df['RF time prediction'], unit='s')\n",
    "mem_report()\n",
    "p.cpu_percent(interval=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_df.to_csv('output_log.csv', axis=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deeper look into the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction accuracy per class\n",
    "bad_pred_list = []\n",
    "for pred in test_df['Next event'].unique():\n",
    "    df_pred = test_df.loc[test_df['Next event'] == pred] \n",
    "    LSTM_acc = round(np.mean(df_pred['Next event'] == df_pred['LSTM event prediction']),3) * 100\n",
    "    if LSTM_acc < 20: # Save classes where prediction accuracy is less than 20%\n",
    "        bad_pred_list.append(pred)\n",
    "    print(\"%s: %s%%\" % (pred, LSTM_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bad_pred = test_df.loc[test_df['Next event'].isin(bad_pred_list)][['Next event', 'LSTM event prediction']]\n",
    "predictions, counts = np.unique(df_bad_pred['LSTM event prediction'], return_counts=True)\n",
    "for i, pred in enumerate(predictions):\n",
    "    print(\"%s: %s\" % (pred, counts[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Actual next event = W_Nabellen offertes: %s \\nLSTM prediction = W_Nabellen offertes: %s\" %\n",
    "(len(test_df.loc[test_df['Next event'] == 'W_Nabellen offertes']), len(test_df.loc[test_df['LSTM event prediction'] == 'W_Nabellen offertes'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.loc[test_df['Case ID'] == 202656][['Case ID', 'Next event', 'LSTM event prediction']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The problem with many occurences of the same activity \n",
    "test_df.loc[test_df['Case ID'] == 202659][['Case ID', 'Next event', 'LSTM event prediction']]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0fafe8da2469ce22b82e1babda22c546ed40097b8f4cbd3e26d6446f22e5d370"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
