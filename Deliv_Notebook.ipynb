{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBL Process Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Class definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Utils.LogFile import LogFile\n",
    "import tensorflow as tf\n",
    "import multiprocessing as mp\n",
    "import copy\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two different methods: \n",
    "- One csv file, which still has to be split into training and test data\n",
    "- Two csv files, which are already split into training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define attribute columns here\n",
    "case_attr = \"Case ID\"\n",
    "act_attr = \"concept:name\"\n",
    "time_attr = \"Complete Timestamp\"\n",
    "path = \"data/BPI_Challenge_2012_end.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create k-context: 5\n"
     ]
    }
   ],
   "source": [
    "logfile = LogFile(path, \",\", 0, None, time_attr=time_attr, trace_attr=case_attr,\n",
    "                   activity_attr=act_attr, convert=False, k=5)\n",
    "logfile.int_convert()\n",
    "logfile.create_k_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data lost due to overlap: 0.07892765488543384/n Best Split: 71\n"
     ]
    }
   ],
   "source": [
    "train_log, test_log = logfile.split_train_test(range(65, 75))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = \"Data/sub_dataset.csv\"\n",
    "#baseline_log = LogFile(path, \",\", 0, None, time_attr='event time:timestamp', trace_attr=case_attr,\n",
    "#                    activity_attr=act_attr, convert=False, k=3)\n",
    "\n",
    "#train_base_log, test_base_log = baseline_log.splitTrainTest(65, split_case=False, method=\"test-train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = \"Data/BPI_Challenge_2012-test.csv\"\n",
    "\n",
    "#baseline_log = LogFile(path, \",\", 0, None, time_attr='event time:timestamp', trace_attr=case_attr,\n",
    "#                    activity_attr=act_attr, convert=False, k=3)\n",
    "\n",
    "#train_base_log, test_base_log = baseline_log.splitTrainTest(70, split_case=False, method=\"test-train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path_train = \"Data/sub_data_train.csv\" \n",
    "#path_test = \"Data/sub_data_test.csv\"\n",
    "\n",
    "#path_train = 'Data/BPI_Challenge_2012-training.csv'\n",
    "#path_test = 'Data\\BPI_Challenge_2012-test.csv'\n",
    "\n",
    "train_base_log = LogFile(path_train, \",\", 0, None, time_attr=time_attr, trace_attr=case_attr,\n",
    "                   activity_attr=act_attr, convert=False, k=3)\n",
    "test_base_log = LogFile(path_test, \",\", 0, None, time_attr=time_attr, trace_attr=case_attr,\n",
    "                    activity_attr=act_attr, convert=False, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_transform(df_train, df_test):\n",
    "    # Standard Scaler \n",
    "    scalar = StandardScaler()\n",
    "    scalar.fit(df_train)\n",
    "    \n",
    "    # Scalar transformation\n",
    "    df_train_transform = scalar.transform(df_train)\n",
    "    df_test_transform = scalar.transform(df_test)\n",
    "\n",
    "    # PCA\n",
    "    df_train_transform = pca.transform(df_train_transform)\n",
    "    df_test_transform = pca.transform(df_test_transform)\n",
    "\n",
    "    return df_train_transform, df_test_transform\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# only keep the starting and end point of a case \n",
    "def split_train_test(path, split_interval):\n",
    "    df = pd.read_csv(path)\n",
    "    data = df[(df['concept:name'] == 'A_SUBMITTED-COMPLETE') | (df['concept:name'] == 'End-End')]\n",
    "    data['time:timestamp'] = pd.to_datetime(data['time:timestamp'])\n",
    "\n",
    "\n",
    "    \n",
    "    loss = len(data)\n",
    "    \n",
    "    for i in split_interval:\n",
    "        train, test = train_test_split(data['Case ID'].unique(), test_size=(100-i)/100, shuffle=False)\n",
    "        train_data = data[data['Case ID'].isin(train)]\n",
    "        test_data = data[data['Case ID'].isin(test)]\n",
    "        \n",
    "        overlap = train_data[train_data['Complete Timestamp'] > test_data['Complete Timestamp'].min()][['Case ID']]\n",
    "        \n",
    "        if len(overlap) < loss:\n",
    "            loss = len(overlap)\n",
    "            best_train = train_data[~train_data['Case ID'].isin(overlap['Case ID'].tolist())]\n",
    "            best_test = test_data\n",
    "    \n",
    "    \n",
    "        print('Train data lost due to overlap: ' + str(len(overlap)/len(train_data)))\n",
    "        return best_train, best_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = split_train_test('data/BPI_2012_Converted.csv', range(67, 73))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(path_train) \n",
    "train_img = scaler.transform(path_train)\n",
    "test_img = scaler.transform(path_test)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(.95)\n",
    "\n",
    "pca.fit(path_train)\n",
    "\n",
    "train_img = pca.transform(path_train)\n",
    "test_img = pca.transform(path_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_time(dataset):\n",
    "    \"\"\"Adds a new column to a dataset with the converted timestamp to datetime\"\"\"\n",
    "\n",
    "    date_list = []\n",
    "\n",
    "    for time in dataset['event time:timestamp']:\n",
    "        datex = time[:-4]\n",
    "        date = datetime.strptime(datex, '%d-%m-%Y %H:%M:%S')\n",
    "\n",
    "        date_list.append(date)\n",
    "\n",
    "    dataset['time and date'] = date_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add actual next event and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_actual_next(df_case):\n",
    "    \"\"\"Adds the actual next activity and time to next event to the final dataframe\"\"\"\n",
    "\n",
    "\n",
    "    # Create a list for all the actual next events for an case\n",
    "    event_lst = [event for event in df_case['event concept:name']] # Gets a list of all events for a specific trace\n",
    "    event_lst = event_lst[1:] # Erase the first activity from the list (thus the second activity becomes first in the list)\n",
    "    event_lst.append('-') # Append a '-' to the end of the list (the last activity does not have a next activity)\n",
    "    \n",
    "    # Create a list for time of the next event\n",
    "    nexttime_lst1 = [time for time in df_case['time and date']]\n",
    "    nexttime_lst = nexttime_lst1[1:]\n",
    "    nexttime_lst.append(nexttime_lst[-1])\n",
    "\n",
    "    # Create the time difference list\n",
    "    time_diff = []\n",
    "    for i in range(len(nexttime_lst)):\n",
    "        time_diff.append((nexttime_lst[i] - nexttime_lst1[i]).total_seconds())\n",
    "\n",
    "    # Append columns to the case dataframe\n",
    "    df_case['Next event'] = event_lst\n",
    "    df_case['Time to next event'] = time_diff\n",
    "\n",
    "    trace_len = len(df_case)\n",
    "\n",
    "    return trace_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicted next event and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_position_time(df_case, count_dict, time_dict):\n",
    "    for index, row in df_case.iterrows():\n",
    "        \n",
    "        # Get the amount of times an action occured in a certain position {action : {position_1 : count_1, position_2: count_2}}\n",
    "        if row['event concept:name'] in count_dict:\n",
    "            if index in count_dict[row['event concept:name']]:\n",
    "                count_dict[row['event concept:name']][index] += 1\n",
    "            else:\n",
    "                count_dict[row['event concept:name']].update({index: 1})\n",
    "        else:\n",
    "            count_dict[row['event concept:name']] = {index: 1}\n",
    "        \n",
    "        # Summation of the times to next action per position (index) {position: {\"sum\": summation_of_time, \"count\": amount_of_times_occured (to calculate mean)}}\n",
    "        if index in time_dict:\n",
    "            time_dict[index]['sum'] += row['Time to next event']\n",
    "            time_dict[index]['count'] += 1\n",
    "        else:\n",
    "            time_dict[index] = {'sum': row['Time to next event'], 'count': 1}\n",
    "\n",
    "def get_position_rank(max_trace_len, count_dict):\n",
    "    pos_rank_dict = {}\n",
    "    for i in range(max_trace_len):\n",
    "        init = 0\n",
    "        task = 0\n",
    "        for key in count_dict.keys():\n",
    "            try:\n",
    "                new = count_dict[key][i]\n",
    "            except:\n",
    "                new = 0\n",
    "            if new > init:\n",
    "                init = new\n",
    "                task = key\n",
    "\n",
    "        pos_rank_dict.update({i: task})\n",
    "    \n",
    "    return pos_rank_dict\n",
    "\n",
    "def get_mean_time(total_time_dict):\n",
    "    mean_time_dict = {}\n",
    "    for position in total_time_dict.keys():\n",
    "        mean_time = total_time_dict[position]['sum'] / total_time_dict[position]['count']\n",
    "        mean_time_dict[position] = mean_time\n",
    "    \n",
    "    return mean_time_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_event_pred(df_case, pos_rank_dict, mean_time_dict):\n",
    "    \n",
    "    # Prediction for the action\n",
    "    pred_act_lst = [pos_rank_dict[i] for i in range(len(df_case))]\n",
    "    pred_act_lst = pred_act_lst[1:]\n",
    "    pred_act_lst.append('-')\n",
    "\n",
    "    # Prediction for time\n",
    "    pred_time_lst = [mean_time_dict[i] for i in range(len(df_case))]\n",
    "\n",
    "    df_case['Event prediction'] = pred_act_lst \n",
    "    df_case['Time prediction'] = pred_time_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_baseline(dataframe, maximum=None):\n",
    "    \"\"\"Returns the training dataset with predictions and 2 dictionaries which predict next action and nexttime based on position\"\"\"\n",
    "    \n",
    "    dataset = dataframe\n",
    "    convert_time(dataset)\n",
    "\n",
    "    df_actual = pd.DataFrame()\n",
    "\n",
    "\n",
    "    # Creating a dataframe with the actual events\n",
    "\n",
    "    cases = list(dataset['case concept:name'].unique())  \n",
    "    max_trace_len = 0  \n",
    "    pos_count_dict = {}\n",
    "    time_dict = {}\n",
    "    for case in cases[:maximum]:\n",
    "        df_case = dataset[dataset['case concept:name'] == case].copy().reset_index(drop=True)\n",
    "        trace_len = add_actual_next(df_case)\n",
    "        get_position_time(df_case, pos_count_dict, time_dict)\n",
    "        df_actual = pd.concat([df_actual, df_case])\n",
    "\n",
    "        if trace_len > max_trace_len:\n",
    "            max_trace_len = trace_len\n",
    "    \n",
    "\n",
    "\n",
    "    # Creating the predicitions\n",
    "    df_predicted = pd.DataFrame()\n",
    "    \n",
    "    pos_rank_dict = get_position_rank(max_trace_len, pos_count_dict)\n",
    "    mean_time_dict = get_mean_time(time_dict)\n",
    "\n",
    "    for case in cases[:maximum]:\n",
    "        df_case = df_actual[df_actual['case concept:name'] == case].copy().reset_index(drop=True)\n",
    "        create_event_pred(df_case, pos_rank_dict, mean_time_dict)\n",
    "        df_predicted = pd.concat([df_predicted,df_case])\n",
    "\n",
    "\n",
    "\n",
    "    return df_predicted, pos_rank_dict, mean_time_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_baseline(dataframe, train_pos, train_time):\n",
    "    \"\"\"Creates the test dataset including the predictions based on the training dataset\"\"\"\n",
    "    \n",
    "    dataset = dataframe\n",
    "    convert_time(dataset)\n",
    "\n",
    "    df_predict = pd.DataFrame()\n",
    "    cases = list(dataset['case concept:name'].unique())  \n",
    "    for case in cases:\n",
    "        df_case = dataset[dataset['case concept:name'] == case].copy().reset_index(drop=True)\n",
    "        _ = add_actual_next(df_case)\n",
    "        create_event_pred(df_case, train_pos, train_time)\n",
    "        df_predict = pd.concat([df_predict, df_case])\n",
    "    \n",
    "    return df_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(dataset):\n",
    "    event_accuracy = np.mean(dataset['Next event'] ==  dataset['Event prediction'])\n",
    "    time_accuracy = np.mean(abs(dataset['Time to next event'] - dataset['Time prediction'])) / 86400  # Mean Absolute Error in days\n",
    "    \n",
    "    return event_accuracy, time_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_base_log.get_data()\n",
    "test_df = test_base_log.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, train_pos, train_time = train_baseline(train_df)\n",
    "test_df = test_baseline(test_df, train_pos, train_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_event_acc, train_time_acc = get_accuracy(train_df)\n",
    "test_event_acc, test_time_acc = get_accuracy(test_df)\n",
    "\n",
    "print(test_event_acc, test_time_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_target(df,\n",
    "                  target_column, target_column2, target_column3,\n",
    "                  target_result, target_result2, target_result3\n",
    "                 ):\n",
    "    \"\"\"Add column to df with integers for the target.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    df -- pandas DataFrame.\n",
    "    target_column -- column to map to int, producing\n",
    "                     new Target column.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_mod -- modified DataFrame.\n",
    "    targets -- list of target names.\n",
    "    \"\"\"\n",
    "    df_mod = df.copy()\n",
    "    targets = df_mod[target_column].unique()\n",
    "    map_to_int = {name: n for n, name in enumerate(targets)}\n",
    "    \n",
    "    targets2 = df_mod[target_column3].unique()\n",
    "    map_to_int2 = {name: n for n, name in enumerate(targets2)}\n",
    "    \n",
    "    \n",
    "    df_mod[f\"{target_result}\"] = df_mod[target_column].replace(map_to_int)\n",
    "    df_mod[f\"{target_result2}\"] = df_mod[target_column2].replace(map_to_int)\n",
    "    df_mod[f\"{target_result3}\"] = df_mod[target_column3].replace(map_to_int2)\n",
    "\n",
    "    return (df_mod)\n",
    "\n",
    "train_df = encode_target(train_df,\n",
    "                                           \"event concept:name\", \"Next event\", \"event lifecycle:transition\",\n",
    "                                           \"current state\", \"next state\", \"lifecycle\")\n",
    "train_df['next state'].replace('-', None, inplace=True)\n",
    "train_df = train_df.dropna()\n",
    "\n",
    "test_df = encode_target(test_df,\n",
    "                                           \"event concept:name\", \"Next event\", \"event lifecycle:transition\",\n",
    "                                           \"current state\", \"next state\", \"lifecycle\")\n",
    "test_df['next state'].replace('-', None, inplace=True)\n",
    "test_df = test_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree event prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sum = 0\n",
    "test_sum = 0\n",
    "\n",
    "for i in range(1):\n",
    "    \n",
    "    y = train_df['next state'].astype(int)\n",
    "    X = train_df[['current state', 'lifecycle']].astype(int)\n",
    "    clf = tree.DecisionTreeClassifier(splitter='best', criterion='entropy')\n",
    "    clf = clf.fit(X, y)\n",
    "    \n",
    "    train_df['tree prediction'] = clf.predict(train_df[['current state', 'lifecycle']])\n",
    "    test_df['tree prediction'] = clf.predict(test_df[['current state', 'lifecycle']])\n",
    "    \n",
    "    correct_event = 0 \n",
    "    total = 0\n",
    "    for index, row in test_df.iterrows():\n",
    "        total += 1\n",
    "        if row['next state'] == row['tree prediction']:\n",
    "            correct_event += 1\n",
    "        \n",
    "    accuracy_event = correct_event/total \n",
    "    test_sum += accuracy_event\n",
    "\n",
    "test_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree time prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sum2 = 0\n",
    "test_sum2 = 0\n",
    "\n",
    "for i in range(1):\n",
    "    \n",
    "    y2 = train_df['Time to next event']\n",
    "    X2 = train_df[['current state', 'lifecycle']].astype(int)\n",
    "    clf2 = tree.DecisionTreeClassifier(splitter='best', criterion='entropy')\n",
    "    clf2 = clf2.fit(X2, y2)\n",
    "    \n",
    "    train_df['tree time prediction'] = clf2.predict(train_df[['current state', 'lifecycle']])\n",
    "    test_df['tree time prediction'] = clf2.predict(test_df[['current state', 'lifecycle']])\n",
    "    \n",
    "    correct_event = 0\n",
    "    total = 0\n",
    "    for index, row in test_df.iterrows():\n",
    "        total += 1\n",
    "        correct_event += abs(row['Time to next event'] - row['tree time prediction'])\n",
    "        \n",
    "    test_sum2 += correct_event / total / 86400\n",
    "    \n",
    "test_sum2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_log(log):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    activities = np.unique(log.data[log.activity])\n",
    "    X = np.zeros((len(log.contextdata), log.k, len(activities)+ 7), dtype=np.float32)\n",
    "    y_a = np.zeros((len(log.contextdata), len(activities) + 1), dtype=np.float32)\n",
    "    y_t = np.zeros((len(log.contextdata)), dtype=np.float32)\n",
    "    j = 0\n",
    "    df = log.contextdata\n",
    "    events_this_day = 0\n",
    "    last_event_day = None\n",
    "    time_diff = 0\n",
    "    for row in log.contextdata.iterrows():\n",
    "        \n",
    "            act = getattr(row[1], log.activity)\n",
    "            event_str = getattr(row[1], log.time)\n",
    "            prev_str = getattr(row[1], \"%s_Prev0\" % (log.time))\n",
    "            #prev_1_str = getattr(row[1], \"%s_Prev1\" % (log.time))\n",
    "            event_time = time.strptime(event_str, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "\n",
    "            if prev_str != 0:\n",
    "                prev_time = time.strptime(prev_str, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "                diff_prev_event = datetime.fromtimestamp(time.mktime(event_time)) \\\n",
    "                                          - datetime.fromtimestamp(time.mktime(prev_time))\n",
    "                diff = diff_prev_event.total_seconds()\n",
    "                event_day = prev_time.tm_wday\n",
    "\n",
    "            else: \n",
    "                diff = 0\n",
    "                event_day = None\n",
    "\n",
    "                        \n",
    "            if event_day != None:\n",
    "                if event_day == last_event_day:\n",
    "                    events_this_day += 1\n",
    "                else:\n",
    "                    last_event_day = event_day\n",
    "                    events_this_day = 1\n",
    "            else: \n",
    "                pass\n",
    "    \n",
    "            y_a[j, act] = 1\n",
    "            y_t[j] = diff            \n",
    "\n",
    "            k = 0\n",
    "            \n",
    "            for i in range(log.k -1, -1, -1):\n",
    "                \n",
    "                if getattr(row[1], \"%s_Prev%i\" % (log.activity, i)) != 0: # 0 indicates no activity (first activity is encoded to 1)\n",
    "                    X[j, log.k - i - 1, getattr(row[1], \"%s_Prev%i\" % (log.activity, i))] = 1\n",
    "                X[j, log.k - i - 1, len(activities)+2] = k\n",
    "                X[j, log.k - i - 1, len(activities) + 3] = time_diff # Diff in seconds\n",
    "\n",
    " \n",
    "                str_time = getattr(row[1], \"%s_Prev0\" % (log.time))\n",
    "                if str_time != 0:\n",
    "                    event_time = time.strptime(str_time, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "                    X[j, log.k - i - 1, len(activities) + 4] = event_time.tm_hour # Hour of day\n",
    "                    X[j, log.k - i - 1, len(activities) + 5] = event_time.tm_wday  # Day of the week\n",
    "                else: \n",
    "                    X[j, log.k - i - 1, len(activities) + 4] = 0 # Hour of day\n",
    "                    X[j, log.k - i - 1, len(activities) + 5] = 0  # Day of the week\n",
    "                \n",
    "                X[j, log.k - 1 - 1, len(activities) + 6] = events_this_day\n",
    "\n",
    "    \n",
    "                try:\n",
    "                    prev_str = getattr(row[1], \"%s_Prev1\" % (log.time))\n",
    "                    #print(\"First success!\", prev_str)\n",
    "                    if prev_str != 0:\n",
    "\n",
    "                        prev_time = time.strptime(prev_str, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "                        diff_prev_event = datetime.fromtimestamp(time.mktime(event_time)) \\\n",
    "                                          - datetime.fromtimestamp(time.mktime(prev_time))\n",
    "                        time_diff = diff_prev_event.total_seconds()\n",
    "                        #print(time_diff)\n",
    "\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                        \n",
    "                    X[j, log.k - i - 1, len(activities) + 3] = event_time.tm_hour # Hour of day\n",
    "                    X[j, log.k - i - 1, len(activities) + 4] = event_time.tm_wday  # Day of the week\n",
    "\n",
    "                k += 1\n",
    "\n",
    "            j += 1\n",
    "\n",
    "    return X, y_a, y_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_LSTM(log, epochs=4, early_stop=42):\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "    from tensorflow.keras.layers import Input\n",
    "    from tensorflow.keras.layers import Dense, BatchNormalization, LSTM\n",
    "    from tensorflow.keras.models import Model\n",
    "    from tensorflow.keras.optimizers import Nadam\n",
    "\n",
    "    print(\"Transforming log...\")\n",
    "    X, y_a, y_t = transform_log(log)\n",
    "\n",
    "    # build the model:\n",
    "    print('Build model...')\n",
    "    main_input = Input(shape=(log.k, len(np.unique(log.data[log.activity]))+7), name='main_input')\n",
    "    # train a 2-layer LSTM with one shared layer\n",
    "    l1 = LSTM(100, implementation=2, kernel_initializer='glorot_uniform', return_sequences=True, dropout=0.2)(main_input) # the shared layer\n",
    "    b1 = BatchNormalization()(l1)\n",
    "    l2_1 = LSTM(100, implementation=2, kernel_initializer='glorot_uniform', return_sequences=False, dropout=0.2)(b1) # the layer specialized in activity prediction\n",
    "    b2_1 = BatchNormalization()(l2_1)\n",
    "    l2_2 = LSTM(100, implementation=2, kernel_initializer='glorot_uniform', return_sequences=False, dropout=0.2)(b1) # the layer specialized in time prediction\n",
    "    b2_2 = BatchNormalization()(l2_2)\n",
    "\n",
    "    act_output = Dense(len(np.unique(log.data[log.activity])) + 1, activation='softmax', kernel_initializer='glorot_uniform', name='act_output')(b2_1)\n",
    "    time_output = Dense(1, kernel_initializer='glorot_uniform', name='time_output')(b2_2)\n",
    "\n",
    "\n",
    "    model = Model(inputs=[main_input], outputs=[act_output, time_output])\n",
    "\n",
    "    opt = Nadam(learning_rate=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004, clipvalue=3)\n",
    "\n",
    "    model.compile(loss={'act_output':'categorical_crossentropy', 'time_output': 'mae'}, optimizer=opt)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=early_stop)\n",
    "    model_checkpoint = ModelCheckpoint(os.path.join(\"model\", 'model_{epoch:03d}-{val_loss:.2f}.h5'), monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=False, mode='auto')\n",
    "    lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "    if len(y_a) > 10:\n",
    "        split = 0.2\n",
    "    else:\n",
    "        split = 0\n",
    "\n",
    "    model.fit(X, {'act_output': y_a, 'time_output': y_t}, validation_split=split, verbose=2, callbacks=[early_stopping, lr_reducer], batch_size=log.k, epochs=epochs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, log):\n",
    "    X, y_a, y_t = transform_log(log)\n",
    "    pred_act, pred_time = model.predict(X)\n",
    "    predict_vals = np.argmax(pred_act, axis=1)\n",
    "    pred_time = pred_time.reshape(-1)\n",
    "    #predict_probs = predictions[np.arange(predictions.shape[0]), predict_vals]\n",
    "    expected_vals = np.argmax(y_a, axis=1)\n",
    "    #expected_probs = predictions[np.arange(predictions.shape[0]), expected_vals]\n",
    "    activity_acc = np.mean(expected_vals ==  predict_vals)\n",
    "    mae_time = np.mean(abs(y_t - pred_time)) / 86400\n",
    "    return predict_vals, pred_time, activity_acc, mae_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create k-context: 5\n",
      "Create k-context: 5\n",
      "Created k context\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\20204502\\OneDrive - TU Eindhoven\\Documents\\GitHub\\Process-Mining\\Process-Mining\\Utils\\LogFile.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.data[self.activity] = self.data[self.activity].replace(map_to_int)\n"
     ]
    }
   ],
   "source": [
    "LSTM_map_train = train_log.int_convert()\n",
    "LSTM_map_test = test_log.int_convert()\n",
    "\n",
    "train_log.create_k_context()\n",
    "test_log.create_k_context()\n",
    "\n",
    "\n",
    "print(\"Created k context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming log...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'iterrows'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\20204502\\OneDrive - TU Eindhoven\\Documents\\GitHub\\Process-Mining\\Process-Mining\\Deliv_Notebook.ipynb Cell 51'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/20204502/OneDrive%20-%20TU%20Eindhoven/Documents/GitHub/Process-Mining/Process-Mining/Deliv_Notebook.ipynb#ch0000052?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m train_LSTM(train_log, epochs\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, early_stop\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\20204502\\OneDrive - TU Eindhoven\\Documents\\GitHub\\Process-Mining\\Process-Mining\\Deliv_Notebook.ipynb Cell 48'\u001b[0m in \u001b[0;36mtrain_LSTM\u001b[1;34m(log, epochs, early_stop)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/20204502/OneDrive%20-%20TU%20Eindhoven/Documents/GitHub/Process-Mining/Process-Mining/Deliv_Notebook.ipynb#ch0000049?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptimizers\u001b[39;00m \u001b[39mimport\u001b[39;00m Nadam\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/20204502/OneDrive%20-%20TU%20Eindhoven/Documents/GitHub/Process-Mining/Process-Mining/Deliv_Notebook.ipynb#ch0000049?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTransforming log...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/20204502/OneDrive%20-%20TU%20Eindhoven/Documents/GitHub/Process-Mining/Process-Mining/Deliv_Notebook.ipynb#ch0000049?line=8'>9</a>\u001b[0m X, y_a, y_t \u001b[39m=\u001b[39m transform_log(log)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/20204502/OneDrive%20-%20TU%20Eindhoven/Documents/GitHub/Process-Mining/Process-Mining/Deliv_Notebook.ipynb#ch0000049?line=10'>11</a>\u001b[0m \u001b[39m# build the model:\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/20204502/OneDrive%20-%20TU%20Eindhoven/Documents/GitHub/Process-Mining/Process-Mining/Deliv_Notebook.ipynb#ch0000049?line=11'>12</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mBuild model...\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\20204502\\OneDrive - TU Eindhoven\\Documents\\GitHub\\Process-Mining\\Process-Mining\\Deliv_Notebook.ipynb Cell 47'\u001b[0m in \u001b[0;36mtransform_log\u001b[1;34m(log)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/20204502/OneDrive%20-%20TU%20Eindhoven/Documents/GitHub/Process-Mining/Process-Mining/Deliv_Notebook.ipynb#ch0000048?line=11'>12</a>\u001b[0m last_event_day \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/20204502/OneDrive%20-%20TU%20Eindhoven/Documents/GitHub/Process-Mining/Process-Mining/Deliv_Notebook.ipynb#ch0000048?line=12'>13</a>\u001b[0m time_diff \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/20204502/OneDrive%20-%20TU%20Eindhoven/Documents/GitHub/Process-Mining/Process-Mining/Deliv_Notebook.ipynb#ch0000048?line=13'>14</a>\u001b[0m \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m log\u001b[39m.\u001b[39;49mcontextdata\u001b[39m.\u001b[39;49miterrows():\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/20204502/OneDrive%20-%20TU%20Eindhoven/Documents/GitHub/Process-Mining/Process-Mining/Deliv_Notebook.ipynb#ch0000048?line=15'>16</a>\u001b[0m         act \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(row[\u001b[39m1\u001b[39m], log\u001b[39m.\u001b[39mactivity)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/20204502/OneDrive%20-%20TU%20Eindhoven/Documents/GitHub/Process-Mining/Process-Mining/Deliv_Notebook.ipynb#ch0000048?line=16'>17</a>\u001b[0m         event_str \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(row[\u001b[39m1\u001b[39m], log\u001b[39m.\u001b[39mtime)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'iterrows'"
     ]
    }
   ],
   "source": [
    "model = train_LSTM(train_log, epochs=5, early_stop=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_act, pred_time, acc_act, mae_time = test(model, test_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(pred_act).replace(LSTM_map_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case ID</th>\n",
       "      <th>Activity</th>\n",
       "      <th>Resource</th>\n",
       "      <th>Complete Timestamp</th>\n",
       "      <th>Variant</th>\n",
       "      <th>Variant index</th>\n",
       "      <th>lifecycle:transition</th>\n",
       "      <th>concept:name</th>\n",
       "      <th>AMOUNT_REQ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>173688</td>\n",
       "      <td>A_SUBMITTED-COMPLETE</td>\n",
       "      <td>112</td>\n",
       "      <td>2011-10-01 00:38:44.546</td>\n",
       "      <td>Variant 613</td>\n",
       "      <td>613</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>1</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>173688</td>\n",
       "      <td>A_PARTLYSUBMITTED-COMPLETE</td>\n",
       "      <td>112</td>\n",
       "      <td>2011-10-01 00:38:44.880</td>\n",
       "      <td>Variant 613</td>\n",
       "      <td>613</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>2</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>173688</td>\n",
       "      <td>A_PREACCEPTED-COMPLETE</td>\n",
       "      <td>112</td>\n",
       "      <td>2011-10-01 00:39:37.906</td>\n",
       "      <td>Variant 613</td>\n",
       "      <td>613</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>3</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>173688</td>\n",
       "      <td>W_Completeren aanvraag-SCHEDULE</td>\n",
       "      <td>112</td>\n",
       "      <td>2011-10-01 00:39:38.875</td>\n",
       "      <td>Variant 613</td>\n",
       "      <td>613</td>\n",
       "      <td>SCHEDULE</td>\n",
       "      <td>4</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>173688</td>\n",
       "      <td>W_Completeren aanvraag-START</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-10-01 11:36:46.437</td>\n",
       "      <td>Variant 613</td>\n",
       "      <td>613</td>\n",
       "      <td>START</td>\n",
       "      <td>4</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197682</th>\n",
       "      <td>202635</td>\n",
       "      <td>End-End</td>\n",
       "      <td>End</td>\n",
       "      <td>2012-01-20 16:28:39.215</td>\n",
       "      <td>Variant 1</td>\n",
       "      <td>1</td>\n",
       "      <td>End</td>\n",
       "      <td>17</td>\n",
       "      <td>End</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197755</th>\n",
       "      <td>202644</td>\n",
       "      <td>A_SUBMITTED-COMPLETE</td>\n",
       "      <td>112</td>\n",
       "      <td>2012-01-20 16:46:31.079</td>\n",
       "      <td>Variant 1</td>\n",
       "      <td>1</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>1</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197756</th>\n",
       "      <td>202644</td>\n",
       "      <td>A_PARTLYSUBMITTED-COMPLETE</td>\n",
       "      <td>112</td>\n",
       "      <td>2012-01-20 16:46:31.266</td>\n",
       "      <td>Variant 1</td>\n",
       "      <td>1</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>2</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197757</th>\n",
       "      <td>202644</td>\n",
       "      <td>A_DECLINED-COMPLETE</td>\n",
       "      <td>112</td>\n",
       "      <td>2012-01-20 16:47:05.173</td>\n",
       "      <td>Variant 1</td>\n",
       "      <td>1</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>19</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197758</th>\n",
       "      <td>202644</td>\n",
       "      <td>End-End</td>\n",
       "      <td>End</td>\n",
       "      <td>2012-01-20 16:47:05.173</td>\n",
       "      <td>Variant 1</td>\n",
       "      <td>1</td>\n",
       "      <td>End</td>\n",
       "      <td>17</td>\n",
       "      <td>End</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>161899 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Case ID                         Activity Resource  \\\n",
       "0        173688             A_SUBMITTED-COMPLETE      112   \n",
       "1        173688       A_PARTLYSUBMITTED-COMPLETE      112   \n",
       "2        173688           A_PREACCEPTED-COMPLETE      112   \n",
       "3        173688  W_Completeren aanvraag-SCHEDULE      112   \n",
       "4        173688     W_Completeren aanvraag-START      NaN   \n",
       "...         ...                              ...      ...   \n",
       "197682   202635                          End-End      End   \n",
       "197755   202644             A_SUBMITTED-COMPLETE      112   \n",
       "197756   202644       A_PARTLYSUBMITTED-COMPLETE      112   \n",
       "197757   202644              A_DECLINED-COMPLETE      112   \n",
       "197758   202644                          End-End      End   \n",
       "\n",
       "             Complete Timestamp      Variant  Variant index  \\\n",
       "0       2011-10-01 00:38:44.546  Variant 613            613   \n",
       "1       2011-10-01 00:38:44.880  Variant 613            613   \n",
       "2       2011-10-01 00:39:37.906  Variant 613            613   \n",
       "3       2011-10-01 00:39:38.875  Variant 613            613   \n",
       "4       2011-10-01 11:36:46.437  Variant 613            613   \n",
       "...                         ...          ...            ...   \n",
       "197682  2012-01-20 16:28:39.215    Variant 1              1   \n",
       "197755  2012-01-20 16:46:31.079    Variant 1              1   \n",
       "197756  2012-01-20 16:46:31.266    Variant 1              1   \n",
       "197757  2012-01-20 16:47:05.173    Variant 1              1   \n",
       "197758  2012-01-20 16:47:05.173    Variant 1              1   \n",
       "\n",
       "       lifecycle:transition  concept:name AMOUNT_REQ  \n",
       "0                  COMPLETE             1      20000  \n",
       "1                  COMPLETE             2      20000  \n",
       "2                  COMPLETE             3      20000  \n",
       "3                  SCHEDULE             4      20000  \n",
       "4                     START             4      20000  \n",
       "...                     ...           ...        ...  \n",
       "197682                  End            17        End  \n",
       "197755             COMPLETE             1       5000  \n",
       "197756             COMPLETE             2       5000  \n",
       "197757             COMPLETE            19       5000  \n",
       "197758                  End            17        End  \n",
       "\n",
       "[161899 rows x 9 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_log.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(true_val, predi)\n",
    "\n",
    "# Making accuracy table\n",
    "metrics_dict = metrics.classification_report(true_val, predi, digits=6, output_dict = True)\n",
    "df2 = pd.DataFrame.from_dict(metrics_dict)\n",
    "df2\n",
    "\n",
    "import seaborn as sns\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt = 'g')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.ylabel(\"True class\")\n",
    "plt.xlabel(\"Predicted class\");"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0fafe8da2469ce22b82e1babda22c546ed40097b8f4cbd3e26d6446f22e5d370"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
