{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBL Process Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Class definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Utils.LogFile import LogFile\n",
    "import tensorflow as tf\n",
    "import multiprocessing as mp\n",
    "import copy\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two different methods: \n",
    "- One csv file, which still has to be split into training and test data\n",
    "- Two csv files, which are already split into training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define attribute columns here\n",
    "case_attr = \"case concept:name\"\n",
    "act_attr = \"event concept:name\"\n",
    "time_attr = \"event time:timestamp\"\n",
    "path = \"Data/sub_dataset.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = \"Data/sub_dataset.csv\"\n",
    "#baseline_log = LogFile(path, \",\", 0, None, time_attr='event time:timestamp', trace_attr=case_attr,\n",
    "#                    activity_attr=act_attr, convert=False, k=3)\n",
    "\n",
    "#train_base_log, test_base_log = baseline_log.splitTrainTest(65, split_case=False, method=\"test-train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = \"Data/BPI_Challenge_2012-test.csv\"\n",
    "\n",
    "#baseline_log = LogFile(path, \",\", 0, None, time_attr='event time:timestamp', trace_attr=case_attr,\n",
    "#                    activity_attr=act_attr, convert=False, k=3)\n",
    "\n",
    "#train_base_log, test_base_log = baseline_log.splitTrainTest(70, split_case=False, method=\"test-train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = \"Data/sub_data_train.csv\" \n",
    "path_test = \"Data/sub_data_test.csv\"\n",
    "\n",
    "#path_train = 'Data/BPI_Challenge_2012-training.csv'\n",
    "#path_test = 'Data\\BPI_Challenge_2012-test.csv'\n",
    "\n",
    "train_base_log = LogFile(path_train, \",\", 0, None, time_attr=time_attr, trace_attr=case_attr,\n",
    "                   activity_attr=act_attr, convert=False, k=3)\n",
    "test_base_log = LogFile(path_test, \",\", 0, None, time_attr=time_attr, trace_attr=case_attr,\n",
    "                    activity_attr=act_attr, convert=False, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_scalar(df_train):\n",
    "    scalar = StandardScaler()\n",
    "    scalar.fit(df_train)\n",
    "    return scalar\n",
    "\n",
    "def scalar_transform(df_train, df_test):\n",
    "    df_train_transform = scaler.transform(df_train)\n",
    "    df_test_transform = scaler.transform(df_test)\n",
    "    return df_train_transform, df_test_transform\n",
    "\n",
    "def pca_transform(df_train, df_test):\n",
    "    df_train_transform = pca.transform(df_train)\n",
    "    df_test_transform = pca.transform(df_test)\n",
    "    return df_train_transform, df_test_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(path_train) \n",
    "train_img = scaler.transform(path_train)\n",
    "test_img = scaler.transform(path_test)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(.95)\n",
    "\n",
    "pca.fit(path_train)\n",
    "\n",
    "train_img = pca.transform(path_train)\n",
    "test_img = pca.transform(path_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_time(dataset):\n",
    "    \"\"\"Adds a new column to a dataset with the converted timestamp to datetime\"\"\"\n",
    "\n",
    "    date_list = []\n",
    "\n",
    "    for time in dataset['event time:timestamp']:\n",
    "        datex = time[:-4]\n",
    "        date = datetime.strptime(datex, '%d-%m-%Y %H:%M:%S')\n",
    "\n",
    "        date_list.append(date)\n",
    "\n",
    "    dataset['time and date'] = date_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add actual next event and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_actual_next(df_case):\n",
    "    \"\"\"Adds the actual next activity and time to next event to the final dataframe\"\"\"\n",
    "\n",
    "\n",
    "    # Create a list for all the actual next events for an case\n",
    "    event_lst = [event for event in df_case['event concept:name']] # Gets a list of all events for a specific trace\n",
    "    event_lst = event_lst[1:] # Erase the first activity from the list (thus the second activity becomes first in the list)\n",
    "    event_lst.append('-') # Append a '-' to the end of the list (the last activity does not have a next activity)\n",
    "    \n",
    "    # Create a list for time of the next event\n",
    "    nexttime_lst1 = [time for time in df_case['time and date']]\n",
    "    nexttime_lst = nexttime_lst1[1:]\n",
    "    nexttime_lst.append(nexttime_lst[-1])\n",
    "\n",
    "    # Create the time difference list\n",
    "    time_diff = []\n",
    "    for i in range(len(nexttime_lst)):\n",
    "        time_diff.append((nexttime_lst[i] - nexttime_lst1[i]).total_seconds())\n",
    "\n",
    "    # Append columns to the case dataframe\n",
    "    df_case['Next event'] = event_lst\n",
    "    df_case['Time to next event'] = time_diff\n",
    "\n",
    "    trace_len = len(df_case)\n",
    "\n",
    "    return trace_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicted next event and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_position_time(df_case, count_dict, time_dict):\n",
    "    for index, row in df_case.iterrows():\n",
    "        \n",
    "        # Get the amount of times an action occured in a certain position {action : {position_1 : count_1, position_2: count_2}}\n",
    "        if row['event concept:name'] in count_dict:\n",
    "            if index in count_dict[row['event concept:name']]:\n",
    "                count_dict[row['event concept:name']][index] += 1\n",
    "            else:\n",
    "                count_dict[row['event concept:name']].update({index: 1})\n",
    "        else:\n",
    "            count_dict[row['event concept:name']] = {index: 1}\n",
    "        \n",
    "        # Summation of the times to next action per position (index) {position: {\"sum\": summation_of_time, \"count\": amount_of_times_occured (to calculate mean)}}\n",
    "        if index in time_dict:\n",
    "            time_dict[index]['sum'] += row['Time to next event']\n",
    "            time_dict[index]['count'] += 1\n",
    "        else:\n",
    "            time_dict[index] = {'sum': row['Time to next event'], 'count': 1}\n",
    "\n",
    "def get_position_rank(max_trace_len, count_dict):\n",
    "    pos_rank_dict = {}\n",
    "    for i in range(max_trace_len):\n",
    "        init = 0\n",
    "        task = 0\n",
    "        for key in count_dict.keys():\n",
    "            try:\n",
    "                new = count_dict[key][i]\n",
    "            except:\n",
    "                new = 0\n",
    "            if new > init:\n",
    "                init = new\n",
    "                task = key\n",
    "\n",
    "        pos_rank_dict.update({i: task})\n",
    "    \n",
    "    return pos_rank_dict\n",
    "\n",
    "def get_mean_time(total_time_dict):\n",
    "    mean_time_dict = {}\n",
    "    for position in total_time_dict.keys():\n",
    "        mean_time = total_time_dict[position]['sum'] / total_time_dict[position]['count']\n",
    "        mean_time_dict[position] = mean_time\n",
    "    \n",
    "    return mean_time_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_event_pred(df_case, pos_rank_dict, mean_time_dict):\n",
    "    \n",
    "    # Prediction for the action\n",
    "    pred_act_lst = [pos_rank_dict[i] for i in range(len(df_case))]\n",
    "    pred_act_lst = pred_act_lst[1:]\n",
    "    pred_act_lst.append('-')\n",
    "\n",
    "    # Prediction for time\n",
    "    pred_time_lst = [mean_time_dict[i] for i in range(len(df_case))]\n",
    "\n",
    "    df_case['Event prediction'] = pred_act_lst \n",
    "    df_case['Time prediction'] = pred_time_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_baseline(dataframe, maximum=None):\n",
    "    \"\"\"Returns the training dataset with predictions and 2 dictionaries which predict next action and nexttime based on position\"\"\"\n",
    "    \n",
    "    dataset = dataframe\n",
    "    convert_time(dataset)\n",
    "\n",
    "    df_actual = pd.DataFrame()\n",
    "\n",
    "\n",
    "    # Creating a dataframe with the actual events\n",
    "\n",
    "    cases = list(dataset['case concept:name'].unique())  \n",
    "    max_trace_len = 0  \n",
    "    pos_count_dict = {}\n",
    "    time_dict = {}\n",
    "    for case in cases[:maximum]:\n",
    "        df_case = dataset[dataset['case concept:name'] == case].copy().reset_index(drop=True)\n",
    "        trace_len = add_actual_next(df_case)\n",
    "        get_position_time(df_case, pos_count_dict, time_dict)\n",
    "        df_actual = pd.concat([df_actual, df_case])\n",
    "\n",
    "        if trace_len > max_trace_len:\n",
    "            max_trace_len = trace_len\n",
    "    \n",
    "\n",
    "\n",
    "    # Creating the predicitions\n",
    "    df_predicted = pd.DataFrame()\n",
    "    \n",
    "    pos_rank_dict = get_position_rank(max_trace_len, pos_count_dict)\n",
    "    mean_time_dict = get_mean_time(time_dict)\n",
    "\n",
    "    for case in cases[:maximum]:\n",
    "        df_case = df_actual[df_actual['case concept:name'] == case].copy().reset_index(drop=True)\n",
    "        create_event_pred(df_case, pos_rank_dict, mean_time_dict)\n",
    "        df_predicted = pd.concat([df_predicted,df_case])\n",
    "\n",
    "\n",
    "\n",
    "    return df_predicted, pos_rank_dict, mean_time_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_baseline(dataframe, train_pos, train_time):\n",
    "    \"\"\"Creates the test dataset including the predictions based on the training dataset\"\"\"\n",
    "    \n",
    "    dataset = dataframe\n",
    "    convert_time(dataset)\n",
    "\n",
    "    df_predict = pd.DataFrame()\n",
    "    cases = list(dataset['case concept:name'].unique())  \n",
    "    for case in cases:\n",
    "        df_case = dataset[dataset['case concept:name'] == case].copy().reset_index(drop=True)\n",
    "        _ = add_actual_next(df_case)\n",
    "        create_event_pred(df_case, train_pos, train_time)\n",
    "        df_predict = pd.concat([df_predict, df_case])\n",
    "    \n",
    "    return df_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(dataset):\n",
    "    event_accuracy = np.mean(dataset['Next event'] ==  dataset['Event prediction'])\n",
    "    time_accuracy = np.mean(abs(dataset['Time to next event'] - dataset['Time prediction'])) / 86400  # Mean Absolute Error in days\n",
    "    \n",
    "    return event_accuracy, time_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_base_log.get_data()\n",
    "test_df = test_base_log.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, train_pos, train_time = train_baseline(train_df)\n",
    "test_df = test_baseline(test_df, train_pos, train_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47865303668069753 0.6874050509292073\n"
     ]
    }
   ],
   "source": [
    "train_event_acc, train_time_acc = get_accuracy(train_df)\n",
    "test_event_acc, test_time_acc = get_accuracy(test_df)\n",
    "\n",
    "print(test_event_acc, test_time_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_target(df,\n",
    "                  target_column, target_column2, target_column3,\n",
    "                  target_result, target_result2, target_result3\n",
    "                 ):\n",
    "    \"\"\"Add column to df with integers for the target.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    df -- pandas DataFrame.\n",
    "    target_column -- column to map to int, producing\n",
    "                     new Target column.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_mod -- modified DataFrame.\n",
    "    targets -- list of target names.\n",
    "    \"\"\"\n",
    "    df_mod = df.copy()\n",
    "    targets = df_mod[target_column].unique()\n",
    "    map_to_int = {name: n for n, name in enumerate(targets)}\n",
    "    \n",
    "    targets2 = df_mod[target_column3].unique()\n",
    "    map_to_int2 = {name: n for n, name in enumerate(targets2)}\n",
    "    \n",
    "    \n",
    "    df_mod[f\"{target_result}\"] = df_mod[target_column].replace(map_to_int)\n",
    "    df_mod[f\"{target_result2}\"] = df_mod[target_column2].replace(map_to_int)\n",
    "    df_mod[f\"{target_result3}\"] = df_mod[target_column3].replace(map_to_int2)\n",
    "\n",
    "    return (df_mod)\n",
    "\n",
    "train_df = encode_target(train_df,\n",
    "                                           \"event concept:name\", \"Next event\", \"event lifecycle:transition\",\n",
    "                                           \"current state\", \"next state\", \"lifecycle\")\n",
    "train_df['next state'].replace('-', None, inplace=True)\n",
    "train_df = train_df.dropna()\n",
    "\n",
    "test_df = encode_target(test_df,\n",
    "                                           \"event concept:name\", \"Next event\", \"event lifecycle:transition\",\n",
    "                                           \"current state\", \"next state\", \"lifecycle\")\n",
    "test_df['next state'].replace('-', None, inplace=True)\n",
    "test_df = test_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eventID</th>\n",
       "      <th>case concept:name</th>\n",
       "      <th>case REG_DATE</th>\n",
       "      <th>case AMOUNT_REQ</th>\n",
       "      <th>event concept:name</th>\n",
       "      <th>event lifecycle:transition</th>\n",
       "      <th>event time:timestamp</th>\n",
       "      <th>time and date</th>\n",
       "      <th>Next event</th>\n",
       "      <th>Time to next event</th>\n",
       "      <th>Event prediction</th>\n",
       "      <th>Time prediction</th>\n",
       "      <th>current state</th>\n",
       "      <th>next state</th>\n",
       "      <th>lifecycle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1571958030336</td>\n",
       "      <td>174839</td>\n",
       "      <td>2011-10-05T16:53:32.364+02:00</td>\n",
       "      <td>6000</td>\n",
       "      <td>A_SUBMITTED</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>05-10-2011 16:53:32.364</td>\n",
       "      <td>2011-10-05 16:53:32</td>\n",
       "      <td>A_PARTLYSUBMITTED</td>\n",
       "      <td>0.0</td>\n",
       "      <td>A_PARTLYSUBMITTED</td>\n",
       "      <td>0.524229</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1571958030337</td>\n",
       "      <td>174839</td>\n",
       "      <td>2011-10-05T16:53:32.364+02:00</td>\n",
       "      <td>6000</td>\n",
       "      <td>A_PARTLYSUBMITTED</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>05-10-2011 16:53:32.501</td>\n",
       "      <td>2011-10-05 16:53:32</td>\n",
       "      <td>A_DECLINED</td>\n",
       "      <td>35.0</td>\n",
       "      <td>A_PREACCEPTED</td>\n",
       "      <td>37.274596</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1571958030338</td>\n",
       "      <td>174839</td>\n",
       "      <td>2011-10-05T16:53:32.364+02:00</td>\n",
       "      <td>6000</td>\n",
       "      <td>A_DECLINED</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>05-10-2011 16:54:07.710</td>\n",
       "      <td>2011-10-05 16:54:07</td>\n",
       "      <td>-</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-</td>\n",
       "      <td>7269.049927</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1576252997632</td>\n",
       "      <td>174842</td>\n",
       "      <td>2011-10-05T16:54:58.973+02:00</td>\n",
       "      <td>5000</td>\n",
       "      <td>A_SUBMITTED</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>05-10-2011 16:54:58.973</td>\n",
       "      <td>2011-10-05 16:54:58</td>\n",
       "      <td>A_PARTLYSUBMITTED</td>\n",
       "      <td>1.0</td>\n",
       "      <td>A_PARTLYSUBMITTED</td>\n",
       "      <td>0.524229</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1576252997633</td>\n",
       "      <td>174842</td>\n",
       "      <td>2011-10-05T16:54:58.973+02:00</td>\n",
       "      <td>5000</td>\n",
       "      <td>A_PARTLYSUBMITTED</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>05-10-2011 16:54:59.079</td>\n",
       "      <td>2011-10-05 16:54:59</td>\n",
       "      <td>A_DECLINED</td>\n",
       "      <td>35.0</td>\n",
       "      <td>A_PREACCEPTED</td>\n",
       "      <td>37.274596</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4492535791617</td>\n",
       "      <td>176972</td>\n",
       "      <td>2011-10-14T13:32:47.208+02:00</td>\n",
       "      <td>25000</td>\n",
       "      <td>A_PARTLYSUBMITTED</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>14-10-2011 13:32:47.462</td>\n",
       "      <td>2011-10-14 13:32:47</td>\n",
       "      <td>W_Afhandelen leads</td>\n",
       "      <td>36.0</td>\n",
       "      <td>A_PREACCEPTED</td>\n",
       "      <td>37.274596</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4492535791618</td>\n",
       "      <td>176972</td>\n",
       "      <td>2011-10-14T13:32:47.208+02:00</td>\n",
       "      <td>25000</td>\n",
       "      <td>W_Afhandelen leads</td>\n",
       "      <td>SCHEDULE</td>\n",
       "      <td>14-10-2011 13:33:23.060</td>\n",
       "      <td>2011-10-14 13:33:23</td>\n",
       "      <td>W_Afhandelen leads</td>\n",
       "      <td>3783.0</td>\n",
       "      <td>W_Completeren aanvraag</td>\n",
       "      <td>7269.049927</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4492535791619</td>\n",
       "      <td>176972</td>\n",
       "      <td>2011-10-14T13:32:47.208+02:00</td>\n",
       "      <td>25000</td>\n",
       "      <td>W_Afhandelen leads</td>\n",
       "      <td>START</td>\n",
       "      <td>14-10-2011 14:36:26.685</td>\n",
       "      <td>2011-10-14 14:36:26</td>\n",
       "      <td>A_DECLINED</td>\n",
       "      <td>356.0</td>\n",
       "      <td>W_Completeren aanvraag</td>\n",
       "      <td>9534.675422</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4492535791620</td>\n",
       "      <td>176972</td>\n",
       "      <td>2011-10-14T13:32:47.208+02:00</td>\n",
       "      <td>25000</td>\n",
       "      <td>A_DECLINED</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>14-10-2011 14:42:22.707</td>\n",
       "      <td>2011-10-14 14:42:22</td>\n",
       "      <td>W_Afhandelen leads</td>\n",
       "      <td>4.0</td>\n",
       "      <td>W_Completeren aanvraag</td>\n",
       "      <td>345.043152</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4492535791621</td>\n",
       "      <td>176972</td>\n",
       "      <td>2011-10-14T13:32:47.208+02:00</td>\n",
       "      <td>25000</td>\n",
       "      <td>W_Afhandelen leads</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>14-10-2011 14:42:26.046</td>\n",
       "      <td>2011-10-14 14:42:26</td>\n",
       "      <td>-</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-</td>\n",
       "      <td>8289.707317</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14665 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         eventID   case concept:name                  case REG_DATE  \\\n",
       "0   1571958030336             174839  2011-10-05T16:53:32.364+02:00   \n",
       "1   1571958030337             174839  2011-10-05T16:53:32.364+02:00   \n",
       "2   1571958030338             174839  2011-10-05T16:53:32.364+02:00   \n",
       "0   1576252997632             174842  2011-10-05T16:54:58.973+02:00   \n",
       "1   1576252997633             174842  2011-10-05T16:54:58.973+02:00   \n",
       "..            ...                ...                            ...   \n",
       "1   4492535791617             176972  2011-10-14T13:32:47.208+02:00   \n",
       "2   4492535791618             176972  2011-10-14T13:32:47.208+02:00   \n",
       "3   4492535791619             176972  2011-10-14T13:32:47.208+02:00   \n",
       "4   4492535791620             176972  2011-10-14T13:32:47.208+02:00   \n",
       "5   4492535791621             176972  2011-10-14T13:32:47.208+02:00   \n",
       "\n",
       "    case AMOUNT_REQ  event concept:name event lifecycle:transition  \\\n",
       "0              6000         A_SUBMITTED                   COMPLETE   \n",
       "1              6000   A_PARTLYSUBMITTED                   COMPLETE   \n",
       "2              6000          A_DECLINED                   COMPLETE   \n",
       "0              5000         A_SUBMITTED                   COMPLETE   \n",
       "1              5000   A_PARTLYSUBMITTED                   COMPLETE   \n",
       "..              ...                 ...                        ...   \n",
       "1             25000   A_PARTLYSUBMITTED                   COMPLETE   \n",
       "2             25000  W_Afhandelen leads                   SCHEDULE   \n",
       "3             25000  W_Afhandelen leads                      START   \n",
       "4             25000          A_DECLINED                   COMPLETE   \n",
       "5             25000  W_Afhandelen leads                   COMPLETE   \n",
       "\n",
       "       event time:timestamp       time and date          Next event  \\\n",
       "0   05-10-2011 16:53:32.364 2011-10-05 16:53:32   A_PARTLYSUBMITTED   \n",
       "1   05-10-2011 16:53:32.501 2011-10-05 16:53:32          A_DECLINED   \n",
       "2   05-10-2011 16:54:07.710 2011-10-05 16:54:07                   -   \n",
       "0   05-10-2011 16:54:58.973 2011-10-05 16:54:58   A_PARTLYSUBMITTED   \n",
       "1   05-10-2011 16:54:59.079 2011-10-05 16:54:59          A_DECLINED   \n",
       "..                      ...                 ...                 ...   \n",
       "1   14-10-2011 13:32:47.462 2011-10-14 13:32:47  W_Afhandelen leads   \n",
       "2   14-10-2011 13:33:23.060 2011-10-14 13:33:23  W_Afhandelen leads   \n",
       "3   14-10-2011 14:36:26.685 2011-10-14 14:36:26          A_DECLINED   \n",
       "4   14-10-2011 14:42:22.707 2011-10-14 14:42:22  W_Afhandelen leads   \n",
       "5   14-10-2011 14:42:26.046 2011-10-14 14:42:26                   -   \n",
       "\n",
       "    Time to next event        Event prediction  Time prediction  \\\n",
       "0                  0.0       A_PARTLYSUBMITTED         0.524229   \n",
       "1                 35.0           A_PREACCEPTED        37.274596   \n",
       "2                  0.0                       -      7269.049927   \n",
       "0                  1.0       A_PARTLYSUBMITTED         0.524229   \n",
       "1                 35.0           A_PREACCEPTED        37.274596   \n",
       "..                 ...                     ...              ...   \n",
       "1                 36.0           A_PREACCEPTED        37.274596   \n",
       "2               3783.0  W_Completeren aanvraag      7269.049927   \n",
       "3                356.0  W_Completeren aanvraag      9534.675422   \n",
       "4                  4.0  W_Completeren aanvraag       345.043152   \n",
       "5                  0.0                       -      8289.707317   \n",
       "\n",
       "    current state next state  lifecycle  \n",
       "0               0          1          0  \n",
       "1               1          2          0  \n",
       "2               2          2          0  \n",
       "0               0          1          0  \n",
       "1               1          2          0  \n",
       "..            ...        ...        ...  \n",
       "1               1         20          0  \n",
       "2              20         20          1  \n",
       "3              20          2          2  \n",
       "4               2         20          0  \n",
       "5              20         20          0  \n",
       "\n",
       "[14665 rows x 15 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree event prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24461815995189418"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sum = 0\n",
    "test_sum = 0\n",
    "\n",
    "for i in range(1):\n",
    "    \n",
    "    y = train_df['next state'].astype(int)\n",
    "    X = train_df[['current state', 'lifecycle']].astype(int)\n",
    "    clf = tree.DecisionTreeClassifier(splitter='best', criterion='entropy')\n",
    "    clf = clf.fit(X, y)\n",
    "    \n",
    "    train_df['tree prediction'] = clf.predict(train_df[['current state', 'lifecycle']])\n",
    "    test_df['tree prediction'] = clf.predict(test_df[['current state', 'lifecycle']])\n",
    "    \n",
    "    correct_event = 0 \n",
    "    total = 0\n",
    "    for index, row in test_df.iterrows():\n",
    "        total += 1\n",
    "        if row['next state'] == row['tree prediction']:\n",
    "            correct_event += 1\n",
    "        \n",
    "    accuracy_event = correct_event/total \n",
    "    test_sum += accuracy_event\n",
    "\n",
    "test_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree time prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4153229563372753"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sum2 = 0\n",
    "test_sum2 = 0\n",
    "\n",
    "for i in range(1):\n",
    "    \n",
    "    y2 = train_df['Time to next event']\n",
    "    X2 = train_df[['current state', 'lifecycle']].astype(int)\n",
    "    clf2 = tree.DecisionTreeClassifier(splitter='best', criterion='entropy')\n",
    "    clf2 = clf2.fit(X2, y2)\n",
    "    \n",
    "    train_df['tree time prediction'] = clf2.predict(train_df[['current state', 'lifecycle']])\n",
    "    test_df['tree time prediction'] = clf2.predict(test_df[['current state', 'lifecycle']])\n",
    "    \n",
    "    correct_event = 0\n",
    "    total = 0\n",
    "    for index, row in test_df.iterrows():\n",
    "        total += 1\n",
    "        correct_event += abs(row['Time to next event'] - row['tree time prediction'])\n",
    "        \n",
    "    test_sum2 += correct_event / total / 86400\n",
    "    \n",
    "test_sum2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_log(log):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    activities = np.unique(log.data[log.activity])\n",
    "    X = np.zeros((len(log.contextdata), log.k, len(activities)+ 7), dtype=np.float32)\n",
    "    y_a = np.zeros((len(log.contextdata), len(activities) + 1), dtype=np.float32)\n",
    "    y_t = np.zeros((len(log.contextdata)), dtype=np.float32)\n",
    "    j = 0\n",
    "    df = log.contextdata\n",
    "    events_this_day = 0\n",
    "    last_event_day = None\n",
    "    time_diff = 0\n",
    "    for row in log.contextdata.iterrows():\n",
    "        \n",
    "            act = getattr(row[1], log.activity)\n",
    "            event_str = getattr(row[1], log.time)\n",
    "            prev_str = getattr(row[1], \"%s_Prev0\" % (log.time))\n",
    "            #prev_1_str = getattr(row[1], \"%s_Prev1\" % (log.time))\n",
    "            event_time = time.strptime(event_str, \"%d-%m-%Y %H:%M:%S.%f\")\n",
    "\n",
    "            if prev_str != 0:\n",
    "                prev_time = time.strptime(prev_str, \"%d-%m-%Y %H:%M:%S.%f\")\n",
    "                diff_prev_event = datetime.fromtimestamp(time.mktime(event_time)) \\\n",
    "                                          - datetime.fromtimestamp(time.mktime(prev_time))\n",
    "                diff = diff_prev_event.total_seconds()\n",
    "                event_day = prev_time.tm_wday\n",
    "\n",
    "            else: \n",
    "                diff = 0\n",
    "                event_day = None\n",
    "\n",
    "                        \n",
    "            if event_day != None:\n",
    "                if event_day == last_event_day:\n",
    "                    events_this_day += 1\n",
    "                else:\n",
    "                    last_event_day = event_day\n",
    "                    events_this_day = 1\n",
    "            else: \n",
    "                pass\n",
    "    \n",
    "            y_a[j, act] = 1\n",
    "            y_t[j] = diff            \n",
    "\n",
    "            k = 0\n",
    "            \n",
    "            for i in range(log.k -1, -1, -1):\n",
    "                \n",
    "                if getattr(row[1], \"%s_Prev%i\" % (log.activity, i)) != 0: # 0 indicates no activity (first activity is encoded to 1)\n",
    "                    X[j, log.k - i - 1, getattr(row[1], \"%s_Prev%i\" % (log.activity, i))] = 1\n",
    "                X[j, log.k - i - 1, len(activities)+2] = k\n",
    "                X[j, log.k - i - 1, len(activities) + 3] = time_diff # Diff in seconds\n",
    "\n",
    " \n",
    "                str_time = getattr(row[1], \"%s_Prev0\" % (log.time))\n",
    "                if str_time != 0:\n",
    "                    event_time = time.strptime(str_time, \"%d-%m-%Y %H:%M:%S.%f\")\n",
    "                    X[j, log.k - i - 1, len(activities) + 4] = event_time.tm_hour # Hour of day\n",
    "                    X[j, log.k - i - 1, len(activities) + 5] = event_time.tm_wday  # Day of the week\n",
    "                else: \n",
    "                    X[j, log.k - i - 1, len(activities) + 4] = 0 # Hour of day\n",
    "                    X[j, log.k - i - 1, len(activities) + 5] = 0  # Day of the week\n",
    "                \n",
    "                X[j, log.k - 1 - 1, len(activities) + 6] = events_this_day\n",
    "\n",
    "    \n",
    "                try:\n",
    "                    prev_str = getattr(row[1], \"%s_Prev1\" % (log.time))\n",
    "                    #print(\"First success!\", prev_str)\n",
    "                    if prev_str != 0:\n",
    "\n",
    "                        prev_time = time.strptime(prev_str, \"%d-%m-%Y %H:%M:%S.%f\")\n",
    "                        diff_prev_event = datetime.fromtimestamp(time.mktime(event_time)) \\\n",
    "                                          - datetime.fromtimestamp(time.mktime(prev_time))\n",
    "                        time_diff = diff_prev_event.total_seconds()\n",
    "                        #print(time_diff)\n",
    "\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                        \n",
    "                    X[j, log.k - i - 1, len(activities) + 3] = event_time.tm_hour # Hour of day\n",
    "                    X[j, log.k - i - 1, len(activities) + 4] = event_time.tm_wday  # Day of the week\n",
    "\n",
    "                k += 1\n",
    "\n",
    "            j += 1\n",
    "\n",
    "    return X, y_a, y_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_LSTM(log, epochs=4, early_stop=42):\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "    from tensorflow.keras.layers import Input\n",
    "    from tensorflow.keras.layers import Dense, BatchNormalization, LSTM\n",
    "    from tensorflow.keras.models import Model\n",
    "    from tensorflow.keras.optimizers import Nadam\n",
    "\n",
    "    print(\"Transforming log...\")\n",
    "    X, y_a, y_t = transform_log(log)\n",
    "\n",
    "    # build the model:\n",
    "    print('Build model...')\n",
    "    main_input = Input(shape=(log.k, len(np.unique(log.data[log.activity]))+7), name='main_input')\n",
    "    # train a 2-layer LSTM with one shared layer\n",
    "    l1 = LSTM(100, implementation=2, kernel_initializer='glorot_uniform', return_sequences=True, dropout=0.2)(main_input) # the shared layer\n",
    "    b1 = BatchNormalization()(l1)\n",
    "    l2_1 = LSTM(100, implementation=2, kernel_initializer='glorot_uniform', return_sequences=False, dropout=0.2)(b1) # the layer specialized in activity prediction\n",
    "    b2_1 = BatchNormalization()(l2_1)\n",
    "    l2_2 = LSTM(100, implementation=2, kernel_initializer='glorot_uniform', return_sequences=False, dropout=0.2)(b1) # the layer specialized in time prediction\n",
    "    b2_2 = BatchNormalization()(l2_2)\n",
    "\n",
    "    act_output = Dense(len(np.unique(log.data[log.activity])) + 1, activation='softmax', kernel_initializer='glorot_uniform', name='act_output')(b2_1)\n",
    "    time_output = Dense(1, kernel_initializer='glorot_uniform', name='time_output')(b2_2)\n",
    "\n",
    "\n",
    "    model = Model(inputs=[main_input], outputs=[act_output, time_output])\n",
    "\n",
    "    opt = Nadam(learning_rate=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004, clipvalue=3)\n",
    "\n",
    "    model.compile(loss={'act_output':'categorical_crossentropy', 'time_output': 'mae'}, optimizer=opt)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=early_stop)\n",
    "    model_checkpoint = ModelCheckpoint(os.path.join(\"model\", 'model_{epoch:03d}-{val_loss:.2f}.h5'), monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=False, mode='auto')\n",
    "    lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "    if len(y_a) > 10:\n",
    "        split = 0.2\n",
    "    else:\n",
    "        split = 0\n",
    "\n",
    "    model.fit(X, {'act_output': y_a, 'time_output': y_t}, validation_split=split, verbose=2, callbacks=[early_stopping, lr_reducer], batch_size=log.k, epochs=epochs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, log):\n",
    "    X, y_a, y_t = transform_log(log)\n",
    "    pred_act, pred_time = model.predict(X)\n",
    "    predict_vals = np.argmax(pred_act, axis=1)\n",
    "    pred_time = pred_time.reshape(-1)\n",
    "    #predict_probs = predictions[np.arange(predictions.shape[0]), predict_vals]\n",
    "    expected_vals = np.argmax(y_a, axis=1)\n",
    "    #expected_probs = predictions[np.arange(predictions.shape[0]), expected_vals]\n",
    "    activity_acc = np.mean(expected_vals ==  predict_vals)\n",
    "    mae_time = np.mean(abs(y_t - pred_time)) / 86400\n",
    "    return predict_vals, pred_time, activity_acc, mae_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create k-context: 2\n",
      "Create k-context: 2\n",
      "Created k context\n"
     ]
    }
   ],
   "source": [
    "LSTM_log_train = LogFile(path_train, \",\", 0, None, 'event time:timestamp', case_attr,\n",
    "                    activity_attr=act_attr, convert=False, k=2)\n",
    "LSTM_log_test = LogFile(path_test, \",\", 0, None, 'event time:timestamp', case_attr,\n",
    "                    activity_attr=act_attr, convert=False, k=2)\n",
    "\n",
    "LSTM_map_train = LSTM_log_train.int_convert()\n",
    "LSTM_map_test = LSTM_log_test.int_convert()\n",
    "\n",
    "LSTM_log_train.remove_attributes(['eventID', 'case REG_DATE', 'case AMOUNT_REQ', 'event lifecycle:transition'])\n",
    "LSTM_log_test.remove_attributes(['eventID', 'case REG_DATE', 'case AMOUNT_REQ', 'event lifecycle:transition'])\n",
    "\n",
    "LSTM_log_train.create_k_context()\n",
    "LSTM_log_test.create_k_context()\n",
    "\n",
    "\n",
    "print(\"Created k context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming log...\n",
      "Build model...\n",
      "Epoch 1/2\n",
      "3326/3326 - 23s - loss: 34734.7461 - act_output_loss: 2.5549 - time_output_loss: 34732.1211 - val_loss: 40378.0508 - val_act_output_loss: 2.2559 - val_time_output_loss: 40375.8125 - lr: 0.0020 - 23s/epoch - 7ms/step\n",
      "Epoch 2/2\n",
      "3326/3326 - 13s - loss: 34729.1484 - act_output_loss: 2.3778 - time_output_loss: 34726.7578 - val_loss: 40379.9219 - val_act_output_loss: 2.2634 - val_time_output_loss: 40377.6992 - lr: 0.0020 - 13s/epoch - 4ms/step\n"
     ]
    }
   ],
   "source": [
    "model = train_LSTM(LSTM_log_test, epochs=2, early_stop=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_act, pred_time, acc_act, mae_time = test(model, LSTM_log_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A_SUBMITTED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A_PARTLYSUBMITTED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A_PARTLYSUBMITTED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A_SUBMITTED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A_PARTLYSUBMITTED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14660</th>\n",
       "      <td>W_Completeren aanvraag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14661</th>\n",
       "      <td>W_Nabellen incomplete dossiers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14662</th>\n",
       "      <td>W_Completeren aanvraag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14663</th>\n",
       "      <td>W_Completeren aanvraag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14664</th>\n",
       "      <td>W_Completeren aanvraag</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14665 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    0\n",
       "0                         A_SUBMITTED\n",
       "1                   A_PARTLYSUBMITTED\n",
       "2                   A_PARTLYSUBMITTED\n",
       "3                         A_SUBMITTED\n",
       "4                   A_PARTLYSUBMITTED\n",
       "...                               ...\n",
       "14660          W_Completeren aanvraag\n",
       "14661  W_Nabellen incomplete dossiers\n",
       "14662          W_Completeren aanvraag\n",
       "14663          W_Completeren aanvraag\n",
       "14664          W_Completeren aanvraag\n",
       "\n",
       "[14665 rows x 1 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(pred_act).replace(LSTM_map_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'true_val' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\20204502\\OneDrive - TU Eindhoven\\Documents\\GitHub\\Process-Mining\\Process-Mining\\Deliv_Notebook.ipynb Cell 48'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/20204502/OneDrive%20-%20TU%20Eindhoven/Documents/GitHub/Process-Mining/Process-Mining/Deliv_Notebook.ipynb#ch0000047?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m classification_report, confusion_matrix\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/20204502/OneDrive%20-%20TU%20Eindhoven/Documents/GitHub/Process-Mining/Process-Mining/Deliv_Notebook.ipynb#ch0000047?line=2'>3</a>\u001b[0m cm \u001b[39m=\u001b[39m confusion_matrix(true_val, predi)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/20204502/OneDrive%20-%20TU%20Eindhoven/Documents/GitHub/Process-Mining/Process-Mining/Deliv_Notebook.ipynb#ch0000047?line=4'>5</a>\u001b[0m \u001b[39m# Making accuracy table\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/20204502/OneDrive%20-%20TU%20Eindhoven/Documents/GitHub/Process-Mining/Process-Mining/Deliv_Notebook.ipynb#ch0000047?line=5'>6</a>\u001b[0m metrics_dict \u001b[39m=\u001b[39m metrics\u001b[39m.\u001b[39mclassification_report(true_val, predi, digits\u001b[39m=\u001b[39m\u001b[39m6\u001b[39m, output_dict \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'true_val' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(true_val, predi)\n",
    "\n",
    "# Making accuracy table\n",
    "metrics_dict = metrics.classification_report(true_val, predi, digits=6, output_dict = True)\n",
    "df2 = pd.DataFrame.from_dict(metrics_dict)\n",
    "df2\n",
    "\n",
    "import seaborn as sns\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt = 'g')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.ylabel(\"True class\")\n",
    "plt.xlabel(\"Predicted class\");"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0fafe8da2469ce22b82e1babda22c546ed40097b8f4cbd3e26d6446f22e5d370"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
